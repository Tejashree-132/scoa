{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- Load your CSV dataset ---\n",
        "df = pd.read_csv(\"/content/SCOA_A4.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\"species\", axis=1).values\n",
        "y = LabelEncoder().fit_transform(df[\"species\"].values)  # convert labels to integers\n",
        "\n",
        "# --- Genetic Algorithm Setup ---\n",
        "POP_SIZE = 20       # number of individuals\n",
        "N_GENERATIONS = 10  # number of generations\n",
        "MUTATION_RATE = 0.2\n",
        "\n",
        "# Chromosome: [max_depth, min_samples_split]\n",
        "def create_chromosome():\n",
        "    return [random.randint(1, 20), random.randint(2, 10)]\n",
        "\n",
        "def fitness(chromosome):\n",
        "    max_depth, min_samples_split = chromosome\n",
        "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
        "                                   min_samples_split=min_samples_split,\n",
        "                                   random_state=42)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    return scores.mean()\n",
        "\n",
        "def selection(population, fitnesses):\n",
        "    # Select the top 2 individuals\n",
        "    idx = np.argsort(fitnesses)[-2:]\n",
        "    return [population[idx[0]], population[idx[1]]]\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    # Single point crossover\n",
        "    point = random.randint(1, len(parent1)-1)\n",
        "    child1 = parent1[:point] + parent2[point:]\n",
        "    child2 = parent2[:point] + parent1[point:]\n",
        "    return child1, child2\n",
        "\n",
        "def mutate(chromosome):\n",
        "    if random.random() < MUTATION_RATE:\n",
        "        chromosome[0] = random.randint(1, 20)\n",
        "    if random.random() < MUTATION_RATE:\n",
        "        chromosome[1] = random.randint(2, 10)\n",
        "    return chromosome\n",
        "\n",
        "# --- Initialize population ---\n",
        "population = [create_chromosome() for _ in range(POP_SIZE)]\n",
        "\n",
        "# --- Run GA ---\n",
        "for gen in range(N_GENERATIONS):\n",
        "    fitnesses = [fitness(chromo) for chromo in population]\n",
        "    print(f\"Generation {gen} - Best Fitness: {max(fitnesses):.4f}\")\n",
        "\n",
        "    new_population = []\n",
        "    parents = selection(population, fitnesses)\n",
        "\n",
        "    # Generate next generation\n",
        "    for _ in range(POP_SIZE // 2):\n",
        "        child1, child2 = crossover(parents[0], parents[1])\n",
        "        new_population.append(mutate(child1))\n",
        "        new_population.append(mutate(child2))\n",
        "\n",
        "    population = new_population\n",
        "\n",
        "# --- Best hyperparameters ---\n",
        "fitnesses = [fitness(chromo) for chromo in population]\n",
        "best_idx = np.argmax(fitnesses)\n",
        "print(\"\\nBest Hyperparameters:\", population[best_idx])\n",
        "print(\"Best Accuracy:\", fitnesses[best_idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSejH37gN4YC",
        "outputId": "31623572-3c44-49b2-8b7e-bc69388b497d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0 - Best Fitness: 0.9733\n",
            "Generation 1 - Best Fitness: 0.9733\n",
            "Generation 2 - Best Fitness: 0.9733\n",
            "Generation 3 - Best Fitness: 0.9733\n",
            "Generation 4 - Best Fitness: 0.9733\n",
            "Generation 5 - Best Fitness: 0.9733\n",
            "Generation 6 - Best Fitness: 0.9733\n",
            "Generation 7 - Best Fitness: 0.9733\n",
            "Generation 8 - Best Fitness: 0.9733\n",
            "Generation 9 - Best Fitness: 0.9733\n",
            "\n",
            "Best Hyperparameters: [3, 9]\n",
            "Best Accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Genetic Algorithm (GA)** to optimize **Decision Tree hyperparameters** (specifically `max_depth` and `min_samples_split`) using a dataset called `SCOA_A4.csv`.\n",
        "\n",
        "The goal of this program:\n",
        "\n",
        "> Use a **Genetic Algorithm (GA)** to find the best Decision Tree parameters that yield the **highest classification accuracy** on your dataset.\n",
        "\n",
        "We’re tuning:\n",
        "\n",
        "* `max_depth`: how deep the decision tree can grow.\n",
        "* `min_samples_split`: the minimum number of samples required to split a node.\n",
        "\n",
        "---\n",
        "\n",
        "Step 1: Import Libraries\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "\n",
        "* `numpy` → used for numerical operations (sorting, arrays).\n",
        "* `pandas` → used to read and manipulate your CSV dataset.\n",
        "* `random` → used for random number generation (essential in GA).\n",
        "* `cross_val_score` → performs cross-validation to measure model accuracy.\n",
        "* `DecisionTreeClassifier` → your machine learning model.\n",
        "* `LabelEncoder` → converts categorical labels (like species names) into numeric values.\n",
        "\n",
        "---\n",
        "\n",
        "Step 2: Load and Prepare the Dataset\n",
        "\n",
        "```python\n",
        "df = pd.read_csv(\"/content/SCOA_A4.csv\")\n",
        "```\n",
        "\n",
        "Loads your dataset from Google Colab or local path.\n",
        "Example dataset might look like:\n",
        "\n",
        "| sepal_length | sepal_width | petal_length | petal_width | species    |\n",
        "| ------------ | ----------- | ------------ | ----------- | ---------- |\n",
        "| 5.1          | 3.5         | 1.4          | 0.2         | setosa     |\n",
        "| 6.0          | 3.4         | 4.5          | 1.6         | versicolor |\n",
        "\n",
        "---\n",
        "\n",
        "Split into Features (X) and Target (y)\n",
        "\n",
        "```python\n",
        "X = df.drop(\"species\", axis=1).values\n",
        "y = LabelEncoder().fit_transform(df[\"species\"].values)\n",
        "```\n",
        "\n",
        "* `X`: input features (numerical columns except species)\n",
        "* `y`: encoded class labels (`species` column converted to integers)\n",
        "\n",
        "  * Example: setosa → 0, versicolor → 1, virginica → 2\n",
        "\n",
        "---\n",
        "\n",
        "Step 3: GA Hyperparameters\n",
        "\n",
        "```python\n",
        "POP_SIZE = 20       # number of chromosomes (solutions)\n",
        "N_GENERATIONS = 10  # evolution cycles\n",
        "MUTATION_RATE = 0.2 # 20% chance for mutation\n",
        "```\n",
        "\n",
        "* Each **chromosome** = one possible set of Decision Tree hyperparameters.\n",
        "* We evolve the population for **10 generations**, improving accuracy.\n",
        "* **Mutation rate 0.2** means: 20% chance that a chromosome gene mutates (changes randomly).\n",
        "\n",
        "---\n",
        "\n",
        "Step 4: Chromosome Definition\n",
        "\n",
        "```python\n",
        "def create_chromosome():\n",
        "    return [random.randint(1, 20), random.randint(2, 10)]\n",
        "```\n",
        "\n",
        "Meaning:\n",
        "\n",
        "Each chromosome = `[max_depth, min_samples_split]`\n",
        "\n",
        "* `max_depth` → between 1 and 20\n",
        "* `min_samples_split` → between 2 and 10\n",
        "\n",
        "So examples:\n",
        "\n",
        "```\n",
        "[3, 5], [10, 2], [8, 9]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Step 5: Fitness Function\n",
        "\n",
        "```python\n",
        "def fitness(chromosome):\n",
        "    max_depth, min_samples_split = chromosome\n",
        "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
        "                                   min_samples_split=min_samples_split,\n",
        "                                   random_state=42)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    return scores.mean()\n",
        "```\n",
        "\n",
        "Meaning:\n",
        "\n",
        "* The **fitness function** evaluates how “good” a chromosome is.\n",
        "* Each chromosome’s genes (`max_depth` and `min_samples_split`) are used to train a `DecisionTreeClassifier`.\n",
        "* `cross_val_score(..., cv=5)` performs **5-fold cross-validation**, splitting the dataset into 5 parts to get reliable accuracy.\n",
        "* The mean accuracy is returned as the **fitness value**.\n",
        "\n",
        "mathematical idea:\n",
        "\n",
        "[\n",
        "Fitness(C) = 1/k *  sumation{i=1}^{k} {Accuracy}_i\n",
        "]\n",
        "where ( k = 5 ) folds.\n",
        "\n",
        "---\n",
        "\n",
        "Step 6: Selection\n",
        "\n",
        "```python\n",
        "def selection(population, fitnesses):\n",
        "    idx = np.argsort(fitnesses)[-2:]\n",
        "    return [population[idx[0]], population[idx[1]]]\n",
        "```\n",
        "\n",
        "Meaning:\n",
        "\n",
        "* Selects the **top 2 chromosomes (parents)** with highest fitness (accuracy).\n",
        "* These two will reproduce to create the next generation.\n",
        "\n",
        "---\n",
        "\n",
        "Step 7: Crossover\n",
        "\n",
        "```python\n",
        "def crossover(parent1, parent2):\n",
        "    point = random.randint(1, len(parent1)-1)\n",
        "    child1 = parent1[:point] + parent2[point:]\n",
        "    child2 = parent2[:point] + parent1[point:]\n",
        "    return child1, child2\n",
        "```\n",
        "\n",
        "Meaning:\n",
        "\n",
        "* **Single-point crossover**: choose a random split point and swap genes.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Parent1 = [5, 3]\n",
        "Parent2 = [8, 7]\n",
        "Crossover point = 1\n",
        "→ Child1 = [5, 7]\n",
        "→ Child2 = [8, 3]\n",
        "```\n",
        "\n",
        "So, genetic material (hyperparameters) is exchanged.\n",
        "\n",
        "---\n",
        "\n",
        "Step 8: Mutation\n",
        "\n",
        "```python\n",
        "def mutate(chromosome):\n",
        "    if random.random() < MUTATION_RATE:\n",
        "        chromosome[0] = random.randint(1, 20)\n",
        "    if random.random() < MUTATION_RATE:\n",
        "        chromosome[1] = random.randint(2, 10)\n",
        "    return chromosome\n",
        "```\n",
        "\n",
        "Meaning:\n",
        "\n",
        "* Introduces randomness (biological mutation).\n",
        "* With 20% probability, a chromosome’s gene is replaced with a random value.\n",
        "* Helps the algorithm **escape local optima** (i.e., prevents getting stuck at suboptimal results).\n",
        "\n",
        "---\n",
        "\n",
        "Step 9: Initialize Population\n",
        "\n",
        "```python\n",
        "population = [create_chromosome() for _ in range(POP_SIZE)]\n",
        "```\n",
        "\n",
        "Creates the **initial random population** of 20 chromosomes.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "[[3, 5], [10, 2], [6, 7], [18, 9], ...]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Step 10: Run the Genetic Algorithm\n",
        "\n",
        "```python\n",
        "for gen in range(N_GENERATIONS):\n",
        "    fitnesses = [fitness(chromo) for chromo in population]\n",
        "    print(f\"Generation {gen} - Best Fitness: {max(fitnesses):.4f}\")\n",
        "\n",
        "    new_population = []\n",
        "    parents = selection(population, fitnesses)\n",
        "\n",
        "    for _ in range(POP_SIZE // 2):\n",
        "        child1, child2 = crossover(parents[0], parents[1])\n",
        "        new_population.append(mutate(child1))\n",
        "        new_population.append(mutate(child2))\n",
        "\n",
        "    population = new_population\n",
        "```\n",
        "\n",
        "Meaning (step-by-step):\n",
        "\n",
        "1. **Compute fitness** of each chromosome.\n",
        "2. **Display best fitness** of that generation.\n",
        "3. **Select top 2 parents**.\n",
        "4. **Perform crossover and mutation** to create children.\n",
        "5. **Replace old population** with new generation.\n",
        "\n",
        "This process repeats for 10 generations, simulating *evolution*.\n",
        "\n",
        "---\n",
        "\n",
        "Step 11: Best Hyperparameters\n",
        "\n",
        "```python\n",
        "fitnesses = [fitness(chromo) for chromo in population]\n",
        "best_idx = np.argmax(fitnesses)\n",
        "print(\"\\nBest Hyperparameters:\", population[best_idx])\n",
        "print(\"Best Accuracy:\", fitnesses[best_idx])\n",
        "```\n",
        "\n",
        "Finds the **best chromosome** (highest accuracy) after all generations.\n",
        "\n",
        "---\n",
        "\n",
        "Output Explanation\n",
        "\n",
        "Given Output:\n",
        "\n",
        "```\n",
        "Generation 0 - Best Fitness: 0.9733\n",
        "Generation 1 - Best Fitness: 0.9733\n",
        "Generation 2 - Best Fitness: 0.9733\n",
        "Generation 3 - Best Fitness: 0.9733\n",
        "Generation 4 - Best Fitness: 0.9733\n",
        "Generation 5 - Best Fitness: 0.9733\n",
        "Generation 6 - Best Fitness: 0.9733\n",
        "Generation 7 - Best Fitness: 0.9733\n",
        "Generation 8 - Best Fitness: 0.9733\n",
        "Generation 9 - Best Fitness: 0.9733\n",
        "\n",
        "Best Hyperparameters: [3, 9]\n",
        "Best Accuracy: 0.9733333333333334\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* **Generation 0 → 9:**\n",
        "  The best accuracy (`fitness`) remains **0.9733**.\n",
        "  This means from the very first generation, a nearly optimal solution was already found.\n",
        "\n",
        "* **Best Hyperparameters:** `[3, 9]`\n",
        "\n",
        "  * `max_depth = 3` (tree depth = 3 levels)\n",
        "  * `min_samples_split = 9` (a node splits only if ≥9 samples)\n",
        "\n",
        "* **Best Accuracy:** `0.9733` (≈ 97.33%)\n",
        "\n",
        "  * This is the mean cross-validation accuracy across 5 folds.\n",
        "\n",
        "---\n",
        "\n",
        "Why Accuracy Did Not Improve?\n",
        "\n",
        "1. The dataset might be **small or well-structured**, so early solutions were already optimal.\n",
        "2. GA population (20) and generations (10) are small — but still sufficient for a simple parameter space.\n",
        "3. Mutation and crossover may not find better combinations since `[3, 9]` already gives near-perfect accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "Concept Summary Diagram\n",
        "\n",
        "```\n",
        "┌──────────────────────────────────────────────┐\n",
        "│                Genetic Algorithm             │\n",
        "├──────────────────────────────────────────────┤\n",
        "│ 1. Initialize random population              │\n",
        "│ 2. Evaluate fitness (Decision Tree Accuracy) │\n",
        "│ 3. Select best parents                       │\n",
        "│ 4. Crossover to create children              │\n",
        "│ 5. Mutate some children                      │\n",
        "│ 6. Form new population                       │\n",
        "│ 7. Repeat for N generations                  │\n",
        "└──────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Final Summary\n",
        "\n",
        "| Step                  | Function                                                   | Purpose |\n",
        "| --------------------- | ---------------------------------------------------------- | ------- |\n",
        "| `create_chromosome()` | Creates a random solution `[max_depth, min_samples_split]` |         |\n",
        "| `fitness()`           | Measures model accuracy via cross-validation               |         |\n",
        "| `selection()`         | Chooses top 2 best-performing parents                      |         |\n",
        "| `crossover()`         | Mixes genes of parents to produce offspring                |         |\n",
        "| `mutate()`            | Randomly changes genes to maintain diversity               |         |\n",
        "| GA loop               | Evolves population to maximize accuracy                    |         |\n",
        "| Output                | Displays best hyperparameters and their accuracy           |         |\n",
        "\n",
        "---\n",
        "\n",
        "A Genetic Algorithm (GA) is a nature-inspired optimization technique based on the principles of evolutionary biology and natural selection. The idea was first introduced by John Holland in the 1970s and has since been widely adopted for solving both simple and complex optimization problems. Unlike conventional optimization techniques, GA does not require gradient information or mathematical properties of the problem, making it suitable for non-linear, high-dimensional, and discontinuous search spaces.\n",
        "At its core, GA imitates the process of Darwinian evolution, where stronger individuals in a population are more likely to survive and pass on their traits to the next generation. This is applied in computational systems to iteratively evolve solutions to optimization problems.\n",
        "\n",
        "---\n",
        "1. Representation (Chromosomes)\n",
        "\n",
        "Each potential solution is encoded as a chromosome, often represented as a binary string, integer, or real-valued vector. For example, in machine learning hyperparameter tuning, a chromosome might encode values such as:\n",
        "\n",
        "Learning rate = 0.01\n",
        "\n",
        "Number of hidden layers = 3\n",
        "\n",
        "Regularization parameter = 0.1\n",
        "\n",
        "---\n",
        "2. Population Initialization\n",
        "\n",
        "The algorithm starts with a population of randomly generated chromosomes. The population size is an important parameter, as it determines the diversity of potential solutions in the search space. A larger population increases exploration but requires more computation.\n",
        "\n",
        "---\n",
        "\n",
        "3. Fitness Evaluation\n",
        "\n",
        "Each chromosome is evaluated using a fitness function, which measures the quality of the solution. In the case of machine learning, this may correspond to the accuracy, precision, recall, or mean squared error of the model trained with the given parameters. The better the model performs, the higher the fitness value of its corresponding chromosome.\n",
        "\n",
        "---\n",
        "\n",
        "4. Selection\n",
        "\n",
        "The next step is to select parents from the population based on their fitness. Higher fitness solutions have a greater probability of being selected. Common strategies include:\n",
        "\n",
        "Roulette Wheel Selection (Proportionate Selection): Probability proportional to fitness.\n",
        "\n",
        "Tournament Selection: A subset of chromosomes competes, and the best is selected.\n",
        "\n",
        "Rank Selection: Chromosomes are ranked, and probabilities are assigned accordingly.\n",
        "\n",
        "This ensures that good solutions are more likely to reproduce while maintaining diversity.\n",
        "\n",
        "---\n",
        "\n",
        "5. Crossover (Recombination)\n",
        "\n",
        "Selected parents undergo crossover to produce offspring by exchanging parts of their chromosomes. This mimics biological reproduction and helps combine the strengths of two solutions. Types of crossover include:\n",
        "\n",
        "Single-Point Crossover: A crossover point is chosen, and segments are swapped.\n",
        "\n",
        "Two-Point Crossover: Two crossover points are chosen.\n",
        "\n",
        "Uniform Crossover: Each gene is swapped with some probability.\n",
        "\n",
        "This mechanism introduces new solutions that can potentially perform better than their parents.\n",
        "\n",
        "---\n",
        "\n",
        "6. Mutation\n",
        "\n",
        "To maintain diversity in the population and prevent premature convergence, GA introduces small random changes to chromosomes. For example:\n",
        "\n",
        "Flipping a bit in a binary string.\n",
        "\n",
        "Slightly altering a real-valued parameter.\n",
        "\n",
        "\n",
        "Mutation ensures that the search process explores new regions of the solution space.\n",
        "\n",
        "---\n",
        "\n",
        "7. Replacement (Survivor Selection)\n",
        "\n",
        "After generating offspring, the new generation is formed by replacing some or all members of the old population. Strategies include:\n",
        "\n",
        "Generational Replacement: Entire population replaced.\n",
        "\n",
        "Elitism: Best individuals from the previous generation are carried forward.\n",
        "\n",
        "---\n",
        "8. Termination Criteria\n",
        "\n",
        "The GA continues until a stopping condition is met, such as:\n",
        "A maximum number of generations.\n",
        "\n",
        "No significant improvement in fitness.\n",
        "\n",
        "Achieving a desired accuracy or error threshold.\n",
        "\n",
        "---\n",
        "Application in Machine Learning\n",
        "\n",
        "In machine learning, selecting optimal hyperparameters is often challenging due to the curse of dimensionality and non-linear interactions between parameters. Traditional methods like grid search or random search are computationally expensive and often miss good solutions.\n",
        "\n",
        "By contrast, GA offers the following advantages:\n",
        "Exploration: Searches a wide space of possible parameter values.\n",
        "Adaptation: Learns from previous generations to improve solutions.\n",
        "Efficiency: Finds near-optimal solutions with fewer evaluations than exhaustive search.\n",
        "\n",
        "For instance, in optimizing a Support Vector Machine (SVM):\n",
        "The chromosome may encode kernel type, C, and gamma.\n",
        "GA evaluates each combination using cross-validation accuracy.\n",
        "Over generations, the algorithm converges toward the best set of parameters.\n",
        "\n",
        "---\n",
        "Conclusion:\n",
        "\n",
        "This assignment demonstrates how Genetic Algorithms can optimize machine learning model parameters efficiently. GA provides a robust, adaptive, and intelligent search strategy for hyperparameter tuning, outperforming conventional methods like grid search in terms of flexibility and scalability.\n",
        "By applying GA, the machine learning model achieves higher accuracy and generalization, proving the effectiveness of evolutionary computing in modern AI applications.\n",
        "\n",
        "Sure ✅ — here’s the **Genetic Algorithm (GA)** written in a clear **algorithmic format** suitable for your **SCOA assignment** (with steps, pseudocode, and explanation).\n",
        "\n",
        "---\n",
        "\n",
        "**Genetic Algorithm (GA)**\n",
        "\n",
        "---\n",
        "\n",
        "**Algorithm Steps**\n",
        "\n",
        "**Step 1: Initialization**\n",
        "\n",
        "1. Define the **population size (POP_SIZE)** and **number of generations (N_GENERATIONS)**.\n",
        "2. Define the **chromosome structure** — each chromosome represents one possible solution.\n",
        "   Example:\n",
        "   [\n",
        "   Chromosome = [{max_depth}, {min_samples_split}]\n",
        "   ]\n",
        "3. Randomly initialize the population with possible values.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Fitness Evaluation**\n",
        "\n",
        "1. For each chromosome in the population:\n",
        "\n",
        "   * Decode its genes into actual parameters.\n",
        "   * Evaluate its **fitness** using a performance metric (e.g., model accuracy).\n",
        "     [\n",
        "     Fitness(C) ={Mean Cross-Validation Accuracy}\n",
        "     ]\n",
        "2. Store all fitness values.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Selection**\n",
        "\n",
        "1. Select the best-performing individuals (parents) based on fitness.\n",
        "2. Common methods:\n",
        "\n",
        "   * **Roulette Wheel Selection**\n",
        "   * **Tournament Selection**\n",
        "   * **Elitism (Top-k selection)**\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Crossover (Recombination)**\n",
        "\n",
        "1. Randomly choose a **crossover point** between parent genes.\n",
        "2. Exchange genetic material between parents to produce two children.\n",
        "\n",
        "   Example:\n",
        "\n",
        "   ```\n",
        "   Parent1 = [5, 3]\n",
        "   Parent2 = [8, 7]\n",
        "   Crossover point = 1\n",
        "   → Child1 = [5, 7]\n",
        "   → Child2 = [8, 3]\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Mutation**\n",
        "\n",
        "1. For each child, with a small probability (**MUTATION_RATE**), change one or more genes randomly.\n",
        "   [\n",
        "   {If random() < mutation_rate → change gene to new random value}\n",
        "   ]\n",
        "2. This helps maintain diversity and prevent premature convergence.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 6: New Generation**\n",
        "\n",
        "1. Combine all children to form the **new population**.\n",
        "2. Repeat Steps 2–5 for each generation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 7: Termination**\n",
        "\n",
        "1. Stop when the **maximum number of generations** is reached or the **fitness value converges** (no improvement).\n",
        "2. The **best chromosome** represents the optimal solution.\n",
        "\n",
        "---\n",
        "\n",
        "## **Pseudocode of Genetic Algorithm**\n",
        "\n",
        "```\n",
        "BEGIN\n",
        "  Initialize population P with random chromosomes\n",
        "  FOR generation = 1 to N_GENERATIONS DO\n",
        "      FOR each chromosome Ci in P DO\n",
        "          Compute fitness(Ci)\n",
        "      END FOR\n",
        "\n",
        "      Select best parents (P1, P2) from P\n",
        "      Create empty new_population\n",
        "\n",
        "      WHILE size(new_population) < POP_SIZE DO\n",
        "          Perform crossover on (P1, P2) → (Child1, Child2)\n",
        "          Mutate(Child1)\n",
        "          Mutate(Child2)\n",
        "          Add Child1, Child2 to new_population\n",
        "      END WHILE\n",
        "\n",
        "      Replace P ← new_population\n",
        "      Print best fitness of current generation\n",
        "  END FOR\n",
        "\n",
        "  Return chromosome with highest fitness as optimal solution\n",
        "END\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Step-by-Step Example in Context (Decision Tree Tuning)**\n",
        "\n",
        "| Step | Description                                                       |\n",
        "| ---- | ----------------------------------------------------------------- |\n",
        "| 1    | Randomly create 20 chromosomes like `[3, 5]`, `[10, 2]`, `[6, 8]` |\n",
        "| 2    | Train Decision Tree with these parameters and compute accuracy    |\n",
        "| 3    | Select the top 2 performing chromosomes                           |\n",
        "| 4    | Perform crossover and mutation to generate new children           |\n",
        "| 5    | Replace old population with new one                               |\n",
        "| 6    | Repeat for 10 generations                                         |\n",
        "| 7    | Return the best parameters, e.g., `[3, 9]` with accuracy 0.9733   |\n",
        "\n",
        "---\n",
        "\n",
        "## **Mathematical Representation**\n",
        "\n",
        "1. **Fitness Function:**\n",
        "   [\n",
        "   F(C_i) = 1/k sumation{j=1}^{k} {Accuracy}_{ij}\n",
        "   ]\n",
        "   where ( k ) = number of cross-validation folds.\n",
        "\n",
        "2. **Selection Probability:**\n",
        "   [\n",
        "   P_i = {F(C_i)} / {sumation{j=1}^{N} F(C_j)}\n",
        "   ]\n",
        "\n",
        "3. **Crossover Operation:**\n",
        "   [\n",
        "   {Child1} = P1[0:p] + P2[p:]\n",
        "   ]\n",
        "   [\n",
        "   {Child2} = P2[0:p] + P1[p:]\n",
        "   ]\n",
        "\n",
        "4. **Mutation Operation:**\n",
        "   [\n",
        "   {If random() < rate, then } gene_i = random_value()\n",
        "   ]\n",
        "\n",
        "---\n",
        "\n",
        "## **Advantages of GA**\n",
        "\n",
        "* Works well for **nonlinear**, **complex**, or **multi-modal** problems.\n",
        "* Does **not require gradient** information.\n",
        "* Can escape **local optima** due to mutation.\n",
        "\n",
        "---\n",
        "\n",
        "## **Disadvantages**\n",
        "\n",
        "* Computationally expensive for large populations.\n",
        "* Randomness may slow convergence.\n",
        "* May require tuning of GA parameters (population size, mutation rate, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## **Output Example**\n",
        "\n",
        "```\n",
        "Generation 0 - Best Fitness: 0.9733\n",
        "Generation 1 - Best Fitness: 0.9733\n",
        "...\n",
        "Generation 9 - Best Fitness: 0.9733\n",
        "\n",
        "Best Hyperparameters: [3, 9]\n",
        "Best Accuracy: 0.9733\n",
        "```\n",
        "\n",
        "This means the GA converged quickly to the best solution where:\n",
        "\n",
        "* `max_depth = 3`\n",
        "* `min_samples_split = 9`\n",
        "* Average accuracy = **97.33%**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "gtaGjhfMQPOA"
      }
    }
  ]
}